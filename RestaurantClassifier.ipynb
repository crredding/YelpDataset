{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff32eb60-6869-4466-a9ce-db40edb6780d",
   "metadata": {},
   "source": [
    "# Category classifier\n",
    "The goal of this project is to output a classifier which will predict the review category.\n",
    "\n",
    "**Customer/Use Case:** Potential customer could be Yelp to aid in tagging of businesses automatically using the input review text\n",
    "    * Special care will have to be given to which prediction thresholds to use in order to strike a balance between precision & recall for the given use case.\n",
    "\n",
    "**Approach:** \n",
    "1) Data curation and EDA (accomplished in sperate notebooks)\n",
    "2) Data cleaning\n",
    "    * Reducing feature and data scope (**Initially PA only**)\n",
    "4) Featurize the review data\n",
    "    * First pass will be using **tf-idf**, but additional embeddings could be used as time permits\n",
    "5)  Budilding out classifiers\n",
    "    * **Initial POC** will be binary -- restaurant, not-restaurant -- classifier.\n",
    "    * Secondary will be **multi-class**\n",
    "    * Plan is to test the following ML classifiers:\n",
    "      * Logistic Regression\n",
    "      * Random Forest\n",
    "      * Gradient Boosted\n",
    "      * Other classifiers as time permits\n",
    "6) Evaluate classifers\n",
    "   * See note above about threshold selection\n",
    "7) Deployment\n",
    "   * This is a stretch goal. Would be cool to host on AWS for online input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45fa373-901f-49a6-bee4-63fdf282720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all packages including NLTK downloads as necessary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, RocCurveDisplay, roc_curve, PrecisionRecallDisplay, precision_recall_curve, roc_auc_score\n",
    "first_run = False\n",
    "if first_run:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "import dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07919b85-eef5-472b-9441-7588aba35c94",
   "metadata": {},
   "source": [
    "## Importing and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e812888f-f744-4551-bb5e-8b8f4b11368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_import = True\n",
    "if data_import:\n",
    "    business = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_business.csv\", low_memory=False)\n",
    "    reviews = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_review.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b705ef6-e0f6-45cd-bf91-d7c446d9c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The review cleaning function take a long time. Filtering dataset before we go further.\n",
    "clean_business = dataprep.clean_business_data(business)\n",
    "PA_business = clean_business[clean_business['state'] == 'PA']\n",
    "\n",
    "filtered_reviews = reviews[reviews['business_id'].isin(PA_business['business_id'])].copy()\n",
    "PA_reviews = dataprep.clean_review_data(filtered_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33ea029-f684-44d9-8ea3-e063f70e2cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(business.shape[0])\n",
    "print(PA_business.shape[0])\n",
    "print(reviews.shape[0])\n",
    "print(PA_reviews.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac090c-f597-4230-8ff1-97e5969c0dbb",
   "metadata": {},
   "source": [
    "### Filtering for PA only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9782d464-f43f-440f-90cf-b11f677f552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slicing out only the columns that we may need going forward\n",
    "PA_business = PA_business[['business_id', 'name', 'category_split']]\n",
    "PA_reviews = PA_reviews[['review_id', 'business_id', 'text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cb8bc6-4699-4a65-9dd7-b371ffb81db7",
   "metadata": {},
   "source": [
    "## Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b980b5-db49-4dcf-a298-115e16e0e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_data = PA_business.merge(PA_reviews, how='inner', on='business_id', validate='one_to_many')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1007d32a-d9fa-4b60-a61d-5af866703dc0",
   "metadata": {},
   "source": [
    "### Creating target variable for binary classifier\n",
    "i.e. Restaurant vs. Not-Restaurant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fede6bf-105b-44fe-994f-691c3c0020cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_data['is_restaurant'] = PA_data.apply(lambda row: row['category_split'].count('restaurants') > 0, \n",
    "                                         axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2df1f82-9c3f-4233-a084-247da7e026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The classes here are imbalanced, but there are so many records, that I'm not concerned right now\n",
    "PA_data['is_restaurant'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74734a1d-a145-4a08-a20d-c46374466097",
   "metadata": {},
   "source": [
    "## Getting embeddings from tf-idf for featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18bef5-a3aa-4d2a-86f0-30aafbaab2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying stopwords from multiple sources\n",
    "my_stopwords = ['review']\n",
    "nltk_stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "nltk_stop_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in nltk_stop_words]\n",
    "stopwords = list(set(list(ENGLISH_STOP_WORDS) + my_stopwords + nltk_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d5e729-e0e1-46dd-9057-4f809fa4912a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing words\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        \n",
    "tf = TfidfVectorizer(strip_accents='unicode',\n",
    "                     tokenizer=LemmaTokenizer(),\n",
    "                     stop_words=stopwords,\n",
    "                     max_features=500) # Setting at 500 for POC. Could be tuned further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb706572-aedc-490a-8737-6ae941b1a7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test , y_train, y_test = train_test_split(PA_data['text'].values,\n",
    "                                                     PA_data['is_restaurant'].values, \n",
    "                                                     test_size=0.25, \n",
    "                                                     random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45b2ad-94a9-4535-8db9-a4d41eaa352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use n-grams here\n",
    "tfidf = tf.fit_transform(X_train) #ngram_range=(1, 2)) #Including uni and bi-grams\n",
    "tfidf_test = tf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a598120-3c06-4777-9657-0bcb6735beb8",
   "metadata": {},
   "source": [
    "## Building classifiers\n",
    "Note that I'm not cross-validating my scores for the sake of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23577c62-0de3-4b62-8f6e-ad6a8f95be8f",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe47df-913e-4717-9b9b-bb7595cd960e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LogisticRegressionCV regularlizes by default so we set C really high to avoid this\n",
    "lr = LogisticRegression(random_state=43, C=1000000, max_iter=400)\n",
    "lr.fit(tfidf, y_train)\n",
    "y_pred = lr.predict(tfidf_test)\n",
    "y_pred_score = lr.predict_proba(tfidf_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98f3ffb-5103-444e-bfd4-cd5513d6676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, _ = roc_curve(y_test, y_pred_score[:,1], pos_label=lr.classes_[1])\n",
    "prec, recall, _ = precision_recall_curve(y_test, y_pred_score[:,1], pos_label=lr.classes_[1])\n",
    "auc = roc_auc_score(y_test, y_pred_score[:,1])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc).plot(axs[0])\n",
    "PrecisionRecallDisplay(precision=prec, recall=recall).plot(axs[1])\n",
    "# cm = confusion_matrix(y_test, y_pred)\n",
    "# ConfusionMatrixDisplay(cm).plot()\n",
    "axs[0].plot((0,1), (0,1), ls='--', c='gray')\n",
    "fig.suptitle('Logistic Regression')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088aa52a-50d3-48eb-bfa6-a4bccfd040ae",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1f50a-c1a5-48fe-839d-2d76c11e1d82",
   "metadata": {},
   "source": [
    "This takes a really long time to train, so I'm going to subsample the data and do some averaging for the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60827e60-284b-4e2c-a559-20c2d3c0cd0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm selecting a few subsample sets from the training set, given the size of the data and training time.\n",
    "# Ideally I would do some K-Fold cross-validation here and later hyper-parameter tuning, but for time-constraints\n",
    "# I decided against it.\n",
    "# sklearn.model_selection.ShuffleSplit could be a good way to do this as well\n",
    "num_slices = 5\n",
    "max_dataset = 100000\n",
    "indices_dict = dict()\n",
    "for i in range(num_slices):\n",
    "    indices_dict[i] = np.random.choice(tfidf.shape[0], size=max_dataset, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f1cd29-f5cb-4ec7-88d1-21406e7be67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with default values\n",
    "fprs = []\n",
    "tprs = []\n",
    "precs = []\n",
    "recalls = []\n",
    "aucs = []\n",
    "\n",
    "for i in range(num_slices):\n",
    "    rf = RandomForestClassifier(bootstrap=True, n_jobs=-1, random_state=42)\n",
    "    rf.fit(tfidf[indices_dict[0]], y_train[indices_dict[0]])\n",
    "    y_pred_score = rf.predict_proba(tfidf_test)\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_score[:,1], pos_label=rf.classes_[1])\n",
    "    prec, recall, _ = precision_recall_curve(y_test, y_pred_score[:,1], pos_label=rf.classes_[1])\n",
    "    auc = roc_auc_score(y_test, y_pred_score[:,1])\n",
    "    fprs.append(fpr)\n",
    "    tprs.append(tpr)\n",
    "    precs.append(prec)\n",
    "    recalls.append(recall)\n",
    "    aucs.append(auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59faf31-9f63-4215-8a2c-4e834609e21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_score[:,1])\n",
    "fig, axs = plt.subplots(1, 2, figsize=(8, 3))\n",
    "for i in range(num_slices):\n",
    "    RocCurveDisplay(fpr=fprs[i], tpr=tprs[i], roc_auc=aucs[i]).plot(axs[0])\n",
    "    PrecisionRecallDisplay(precision=precs[i], recall=recalls[i]).plot(axs[1])\n",
    "RocCurveDisplay(fpr=pd.DataFrame(fprs).mean(axis=0).values, \n",
    "            tpr=pd.DataFrame(tprs).mean(axis=0).values, \n",
    "            roc_auc=np.array(aucs).mean()).plot(axs[0], \n",
    "                                                label='Mean AUC = {}'.format(round(np.array(aucs).mean(),\n",
    "                                                                                  2)))\n",
    "axs[0].plot((0,1), (0,1), ls='--', c='gray')\n",
    "fig.suptitle('Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48096b1-f0c2-437c-908b-be4f4f583ea3",
   "metadata": {},
   "source": [
    "# Sample Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b3dcea-53de-4576-9e79-d5c844a38e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = pd.DataFrame(['best burritos in town close to my house service is excellent',\n",
    "              'i hate this place its the worst service ive ever experienced i will never go back',\n",
    "              'i love buying my car parts here',\n",
    "              'ive been going to this dentist for 5 years and every time ive had a good experience'], columns = ['text'])\n",
    "\n",
    "sample_tfidf = tf.transform(test_texts['text'])\n",
    "\n",
    "test_texts['lr_predict'] = lr.predict_proba(sample_tfidf)[:,1]\n",
    "test_texts['rf_predict'] = rf.predict_proba(sample_tfidf)[:,1]\n",
    "test_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeef276-e55f-475a-961f-20c601aae2b5",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "Based on the above, it appears that the LogisticRegression classifier performes better than the RandomForest classifer with an AUC of 0.97 compared to 0.96. However, these are so close, that it could be noise. Both classifiers perform very well over random guessing.\n",
    "\n",
    "There are many ways that this work could be taken further:\n",
    "1) Developing a multi-class classifier (not just restaurants)\n",
    "2) Performing K-fold tests on both models to be sure of the AUC\n",
    "3) Tuning the hyperparameters\n",
    "4) Training on a larger dataset (although this is unlikely to impact the model much)\n",
    "5) Adding other features to the models (zipcode, number of patrons, words in name, review_stars, various other attributes, etc.)\n",
    "6) Featurzing with n-grams instead of only unigrams\n",
    "7) Using different word embeddings (Word2Vec)\n",
    "\n",
    "## Next Steps\n",
    "The next step in the process could be to select an appropriate threshold for prediction before feeding a user a prompt, taking into account the cost of a FP and FN. For example, if we determine that prompting a user to tag a review as \"restaurant\" when it is in fact a gas station, may lead to user churn and or dirty data. If we wish to avoid FPs we may choose a higher Precision over Recall for the final deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43606470-efba-406f-8a98-4b70b0f36d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52a40e0-810e-440d-86a9-1fe61d3e3aad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2eea29-af6a-44a2-96f6-2274f4f79651",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c4e59d-d41d-40f9-807e-77c3ec91de85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd13d03-e3a7-4a78-a369-4ea509429c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_reviews = reviews[reviews['business_id'].isin(filtered_data['business_id'])].copy()\n",
    "# # drop duplicates\n",
    "# filtered_reviews.drop_duplicates('review_id', inplace=True)\n",
    "# # Removing any additional duplicate reviews from same user with same text and business\n",
    "# filtered_reviews.drop_duplicates(['business_id', 'user_id', 'text'], inplace=True)\n",
    "# # filtered_reviews.reset_index(drop=True, inplace=True)\n",
    "# # Cleaning up text field\n",
    "# filtered_reviews['text'] = filtered_reviews['text'].str.lstrip('u').str.strip().str.lower().str.translate(str.maketrans('-', ' ', string.punctuation))\n",
    "# # Removing escaped characters that show up literally\n",
    "# filtered_reviews['text'] = filtered_reviews['text'].str.translate(str.maketrans(dict([(chr(char), ' ') for char in range(1, 32)])))\n",
    "# # Removing >1 whitespace chars\n",
    "# def clean_whitespace(text):\n",
    "#     return ' '.join(text.split())\n",
    "# filtered_reviews['text'] = filtered_reviews.apply(lambda row: clean_whitespace(row['text']), axis=1)\n",
    "# # Dropping few reviews that are NaN\n",
    "# filtered_reviews = filtered_reviews[~filtered_reviews['text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34644404-473d-497c-9565-daccda7bf3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_import = True\n",
    "# if data_import:\n",
    "#     business = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_business.csv\", low_memory=False)\n",
    "# business.drop_duplicates('business_id', inplace=True)\n",
    "# biz_columns_to_drop = ['hours', 'attributes', 'attributes.BusinessParking', 'attributes.HairSpecializesIn', 'attributes.RestaurantsAttire', 'attributes.BestNights',\n",
    "#                        'attributes.Open24Hours', 'attributes.Music', 'attributes.AgesAllowed', 'attributes.BusinessAcceptsCreditCards', 'attributes.DietaryRestrictions']\n",
    "# for column in biz_columns_to_drop:\n",
    "#     business.drop(column, inplace=True, axis=1)\n",
    "# business.reset_index(inplace=True, drop=True)\n",
    "# # Likely not necessary, but updating unicode string import issue leading to leading 'u's\n",
    "# unicode_fix_columns = ['attributes.Alcohol', 'attributes.Smoking', 'attributes.NoiseLevel', 'attributes.BYOBCorkage']\n",
    "\n",
    "# for column in unicode_fix_columns:\n",
    "#     business[column] = business[column].str.lstrip('u').str.strip()\n",
    "\n",
    "# # Update names to strip punctuation and lowercase\n",
    "# business['name'] = business['name'].str.strip().str.lower().str.translate(str.maketrans('', '', string.punctuation))\n",
    "# # Updating corkage to fix duplicate values\n",
    "# business['attributes.BYOBCorkage'] = business['attributes.BYOBCorkage'].str.split('_').str[0].value_counts()\n",
    "\n",
    "# def split_categories(categories):\n",
    "#     try:\n",
    "#         output = categories.split(',')\n",
    "#         return [x.strip().lower() for x in output]\n",
    "#     except:\n",
    "#         return \"\"\n",
    "        \n",
    "# business['category_split'] = business.apply(lambda row: split_categories(row['categories']), axis=1)\n",
    "\n",
    "# categories_to_check = ['restaurants']\n",
    "# business['applicable_categories'] = business['category_split'].apply(lambda x : 1 if any(i in x for i in categories_to_check) else 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

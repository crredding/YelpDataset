{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75865708-751d-4351-8bca-f3e37a2b9a4d",
   "metadata": {},
   "source": [
    "# Category classifier\n",
    "The goal of this project is to generate a restaurant recommender for users. The initial approach is to use unsupervised learnings methods to vectorize restaurants, with similar restaurants having similar space. This may be visualized through PCA. \n",
    "\n",
    "The restaurant vectors will come from the reviews of the various restaurants.\n",
    "\n",
    "A user's feeling about a given restaurant will be taken from either A) their star rating, or B) sentiment analysis from their reviews. I will start with A given its simplicity, however with the caveat that not all 5-star reviews are alike (e.g. I could rate something 5-stars, but not necessarily want to go to a similar place again for various reasons).\n",
    "\n",
    "**Customer/Use Case:** Potential user would be Yelp in order to increase user value of the platform, thereby improving customer aquisition, usage, and retention.\n",
    "\n",
    "**Approach:** \n",
    "1) Data curation and EDA (accomplished in sperate notebooks)\n",
    "2) Data cleaning\n",
    "    * Reducing feature and data scope (**Initially PA only**)\n",
    "    * We would likely only want to categorize restaurants that have a certain number of reviews in order to avoid noisy data.\n",
    "3) Review aggregation and cutoff selection\n",
    "\n",
    "    * All review data will be combined into a single field, with an initial df something like this:\n",
    "| RestaurantId | RestaurantName | AllCombinedReviews |\n",
    "| ------------ | -------------- | ------------------ |\n",
    "| abcde...     | John's Place   | loved it was good, etc. | \n",
    "\n",
    "5) Featurize the review data\n",
    "    * Review data will be features using **tf-idf**, but additional embeddings could be used as time permits.\n",
    "    * Dimensionality reduction will be performed via non-negative matrix factorization\n",
    "      * This will output a reduced feature set for the restaurants. Initial feature set will be 40, but could be tuned as time permits.\n",
    "      * Matrix W will contain cluster centroids. Matrix H will contain cluster membership indicators\n",
    "6)  Budilding out recommender\n",
    "    * **Initial POC** using the business_id from a user's 5-star review, calculate a similiarity score to the other restaurants, and return three restaurants with the highest similarity that do not have the name name (in order to avoid recommended a different Starbucks to someone who likes Starbucks).\n",
    "    * Could test with various similarity scores to see what works best.\n",
    "7) Evaluate recommender\n",
    "   * Evaluation will likely be a manual review given the unsupervised nature of the model\n",
    "8) Deployment\n",
    "   * This is a stretch goal. Would be cool to host on AWS for online input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f3ce1b-16aa-4057-9b13-d5b807a3e938",
   "metadata": {},
   "source": [
    "restaurant to topic (40, hyperparamenter)\n",
    "topic to word (comes from tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "02340c72-7205-40be-a2ab-b674618b182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all packages including NLTK downloads as necessary\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk.corpus\n",
    "import string\n",
    "from sklearn.decomposition import NMF\n",
    "from collections import defaultdict\n",
    "first_run = False\n",
    "if first_run:\n",
    "    nltk.download('stopwords')\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('wordnet')\n",
    "import dataprep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e18b7-4fea-46b4-99e6-faff24b0936d",
   "metadata": {},
   "source": [
    "## Importing and cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bc0f0fe-dcce-4f81-a12d-e897a16e999c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_import = True\n",
    "if data_import:\n",
    "    business = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_business.csv\", low_memory=False)\n",
    "    reviews = pd.read_csv(\"yelp_dataset/yelp_academic_dataset_review.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b5d88c-4540-4f74-96e0-aeb097383a1c",
   "metadata": {},
   "source": [
    "### Filtering for PA restaurants only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a4c9d1e-ea69-43b8-90c3-8653bd23d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The review cleaning function take a long time. Filtering dataset before we go further.\n",
    "clean_business = dataprep.clean_business_data(business)\n",
    "PA_business = clean_business[clean_business['state'] == 'PA']\n",
    "\n",
    "filtered_reviews = reviews[reviews['business_id'].isin(PA_business['business_id'])].copy()\n",
    "PA_reviews = dataprep.clean_review_data(filtered_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fac26ef3-20c0-47e1-9fbc-b5f6ae52f0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chreddin\\AppData\\Local\\Temp\\ipykernel_6192\\3235727762.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PA_business['is_restaurant'] = PA_business.apply(lambda row: row['category_split'].count('restaurants') > 0,\n"
     ]
    }
   ],
   "source": [
    "# Filtering for only restaurants and slicing out only the columns that we may need going forward\n",
    "PA_business['is_restaurant'] = PA_business.apply(lambda row: row['category_split'].count('restaurants') > 0, \n",
    "                                                 axis=1)\n",
    "PA_restaurant = PA_business[PA_business['is_restaurant'] == True].reset_index(drop=True)[['business_id', 'name']]\n",
    "PA_reviews = PA_reviews[['user_id', 'business_id', 'text', 'stars']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8fd572-d75f-4a0c-92b8-4aaeac447396",
   "metadata": {},
   "source": [
    "### Removing reviews for testing\n",
    "I'm doing this here to ensure that review text that I'll eventually test on isn't included in business aggregate reviews\n",
    "\n",
    "**Note:** Another way to evaluate this is to look at star rating for various businesses that users have been to. For example, hold out a set of 5 star reviews for usersand see if my analysis would have predicted that they would have liked the place I recommended based on previous places that they liked. This could be a stretch goal, but a very cool way to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "748b9ed6-a7db-43ff-986e-26bd7aa3fbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold_out_size = 0.25\n",
    "\n",
    "# PA_reviews[PA_reviews['stars'] == 5.0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de976d8c-4424-4510-a089-c637598242bb",
   "metadata": {},
   "source": [
    "## Joining datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60d88a33-c93e-4cc5-b075-bb33582c5919",
   "metadata": {},
   "outputs": [],
   "source": [
    "PA_data = PA_restaurant.merge(PA_reviews, how='inner', on='business_id', validate='one_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e661c5fa-149c-4455-8c01-accadb1cffd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a DataFrame with one row per business with all reviews aggregated into one column\n",
    "PA_combined = PA_data.groupby('business_id', as_index=False).agg({'text':[' '.join, 'count'],\n",
    "                                                                  'name': pd.Series.mode})\n",
    "PA_combined.columns = ['business_id', 'reviews', 'num_reviews', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e30b86c-1cf3-4d9e-bf4b-a84da5f50d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering for businesses that have > min_reviews in order to avoid noisy data.\n",
    "min_reviews = 5\n",
    "PA_combined_filtered = PA_combined[PA_combined['num_reviews'] >= min_reviews].reset_index(drop=True)[['business_id', 'reviews', 'name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "454addca-0e64-421a-a36b-dfab86fc95b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>reviews</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--ZVrH2X2QXBFdCilbirsw</td>\n",
       "      <td>this place is sadly perm closed i was hoping n...</td>\n",
       "      <td>chriss sandwich shop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--epgcb7xHGuJ-4PUeSLAw</td>\n",
       "      <td>love their asiago roll that and a cup of coffe...</td>\n",
       "      <td>manhattan bagel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0FX23yAacC4bbLaGPvyxw</td>\n",
       "      <td>it was our first visit to the restaurant under...</td>\n",
       "      <td>the grey stone fine food and spirits</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0M0b-XhtFagyLmsBtOe8w</td>\n",
       "      <td>review of paris flea market accidentally poppe...</td>\n",
       "      <td>paris wine bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0PN_KFPtbnLQZEeb23XiA</td>\n",
       "      <td>while there didnt seem to be anything wrong wi...</td>\n",
       "      <td>mr wongs chinese restaurant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id                                            reviews  \\\n",
       "0  --ZVrH2X2QXBFdCilbirsw  this place is sadly perm closed i was hoping n...   \n",
       "1  --epgcb7xHGuJ-4PUeSLAw  love their asiago roll that and a cup of coffe...   \n",
       "2  -0FX23yAacC4bbLaGPvyxw  it was our first visit to the restaurant under...   \n",
       "3  -0M0b-XhtFagyLmsBtOe8w  review of paris flea market accidentally poppe...   \n",
       "4  -0PN_KFPtbnLQZEeb23XiA  while there didnt seem to be anything wrong wi...   \n",
       "\n",
       "                                   name  \n",
       "0                  chriss sandwich shop  \n",
       "1                       manhattan bagel  \n",
       "2  the grey stone fine food and spirits  \n",
       "3                        paris wine bar  \n",
       "4           mr wongs chinese restaurant  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PA_combined_filtered.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c06f068-2f63-44ab-9874-95216faef6c3",
   "metadata": {},
   "source": [
    "## Getting embeddings from tf-idf for featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0c9c612-554e-4853-bb7a-98db99b5ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying stopwords from multiple sources\n",
    "my_stopwords = ['review']\n",
    "nltk_stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "nltk_stop_words = [word.translate(str.maketrans('', '', string.punctuation)) for word in nltk_stop_words]\n",
    "stopwords = list(set(list(ENGLISH_STOP_WORDS) + my_stopwords + nltk_stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "afd5836f-7b12-4d4b-9eaf-af9144223c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizing words\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "        \n",
    "tf = TfidfVectorizer(strip_accents='unicode',\n",
    "                     tokenizer=LemmaTokenizer(),\n",
    "                     stop_words=stopwords,\n",
    "                     max_features=500) # Setting at 500 for POC. Could be tuned further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3ef23-aea7-4f7f-9dfe-4e1cd99500e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use train/test split to calculate reconstruction errors using k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086ffe63-2be7-46db-9a59-d2cb763fd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test , y_train, y_test = train_test_split(PA_data['text'].values,\n",
    "#                                                      PA_data['is_restaurant'].values, \n",
    "#                                                      test_size=0.25, \n",
    "#                                                      random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa1c313b-0be8-4ebd-8473-6af7ffa5b933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chreddin\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\chreddin\\AppData\\Local\\anaconda3\\envs\\py312\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['doe', 'ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Could use n-grams here\n",
    "tfidf = tf.fit_transform(PA_combined_filtered['reviews'].values) #ngram_range=(1, 2)) #Including uni and bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08c531f9-db86-4d3b-b880-305d5c5b3585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12641, 500)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b34655ac-b427-4975-94c4-2215070f0796",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee78680-1712-47e9-b586-33a934fd908c",
   "metadata": {},
   "source": [
    "## Fitting NMF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "638f574b-c634-412c-be3c-5b44b5dac048",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=40, max_iter=600) # n_components is being set arbitrarily but could be tuned as time permits\n",
    "nmf.fit(tfidf)\n",
    "\n",
    "H = nmf.components_\n",
    "W = nmf.transform(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "02a71233-addf-42de-9725-ad02082db1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 500)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c32be8e-cf56-441d-a9ac-88b99ea3deec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12641, 40)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeba8e9-6923-48dd-9df4-c770da98f350",
   "metadata": {},
   "source": [
    "### Examining NMF latent features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ae3c0d0f-ed3d-4059-85a4-c465b122b8d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['wa' 'ordered' 'got' 'came' 'went' 'really' 'nice' 'like' 'wanted'\n",
      " 'looked']\n",
      "1 ['pizza' 'crust' 'slice' 'pie' 'cheese' 'good' 'sauce' 'topping' 'great'\n",
      " 'best']\n",
      "2 ['coffee' 'shop' 'good' 'drink' 'great' 'work' 'friendly' 'nice' 'staff'\n",
      " 'chocolate']\n",
      "3 ['chinese' 'food' 'rice' 'shrimp' 'egg' 'good' 'takeout' 'roll' 'fried'\n",
      " 'place']\n",
      "4 ['sushi' 'roll' 'tuna' 'salmon' 'fish' 'spicy' 'fresh' 'good' 'rice'\n",
      " 'great']\n",
      "5 ['bar' 'drink' 'bartender' 'night' 'music' 'great' 'game' 'friend' 'good'\n",
      " 'cocktail']\n",
      "6 ['taco' 'mexican' 'food' 'fish' 'order' 'pork' 'margarita' 'chip' 'good'\n",
      " 'great']\n",
      "7 ['location' 'order' 'time' 'employee' 'customer' 'service' 'line' 'like'\n",
      " 'manager' 'minute']\n",
      "8 ['sandwich' 'bread' 'roll' 'lunch' 'cheese' 'meat' 'great' 'good' 'beef'\n",
      " 'pork']\n",
      "9 ['breakfast' 'egg' 'diner' 'toast' 'pancake' 'bacon' 'french' 'brunch'\n",
      " 'sausage' 'great']\n",
      "10 ['burger' 'bun' 'onion' 'bacon' 'guy' 'good' 'great' 'cheese' 'topping'\n",
      " 'order']\n",
      "11 ['indian' 'curry' 'dish' 'lamb' 'restaurant' 'spice' 'spicy' 'wa' 'good'\n",
      " 'lunch']\n",
      "12 ['mac' 'cheese' 'platter' 'green' 'food' 'fish' 'potato' 'good' 'like'\n",
      " 'fried']\n",
      "13 ['bagel' 'cheese' 'cream' 'breakfast' 'egg' 'sandwich' 'bacon' 'order'\n",
      " 'good' 'service']\n",
      "14 ['pho' 'roll' 'noodle' 'pork' 'beef' 'bun' 'restaurant' 'good' 'dish'\n",
      " 'shrimp']\n",
      "15 ['cake' 'chocolate' 'good' 'birthday' 'sweet' 'pie' 'delicious' 'dessert'\n",
      " 'bun' 'butter']\n",
      "16 ['cheesesteak' 'steak' 'cheese' 'philly' 'meat' 'roll' 'onion' 'good'\n",
      " 'best' 'pepper']\n",
      "17 ['thai' 'curry' 'wa' 'noodle' 'dish' 'food' 'restaurant' 'spicy' 'rice'\n",
      " 'good']\n",
      "18 ['restaurant' 'wa' 'dish' 'menu' 'wine' 'great' 'dinner' 'dessert'\n",
      " 'entree' 'reservation']\n",
      "19 ['crab' 'seafood' 'shrimp' 'lobster' 'fish' 'fried' 'platter' 'potato'\n",
      " 'good' 'sauce']\n",
      "20 ['beer' 'selection' 'great' 'good' 'place' 'food' 'menu' 'list' 'bar'\n",
      " 'atmosphere']\n",
      "21 ['wing' 'sauce' 'good' 'fried' 'garlic' 'great' 'chicken' 'hot' 'place'\n",
      " 'best']\n",
      "22 ['bbq' 'rib' 'pork' 'meat' 'sauce' 'good' 'bean' 'potato' 'mac' 'flavor']\n",
      "23 ['donut' 'cream' 'order' 'fresh' 'coffee' 'hot' 'best' 'like' 'flavor'\n",
      " 'bacon']\n",
      "24 ['chicken' 'rice' 'fried' 'sauce' 'spicy' 'good' 'tender' 'piece' 'lamb'\n",
      " 'meal']\n",
      "25 ['food' 'good' 'great' 'service' 'restaurant' 'friendly' 'delicious' 'eat'\n",
      " 'wa' 'platter']\n",
      "26 ['salad' 'lunch' 'soup' 'fresh' 'good' 'chicken' 'option' 'ingredient'\n",
      " 'bread' 'like']\n",
      "27 ['bowl' 'rice' 'fresh' 'topping' 'option' 'burrito' 'ingredient'\n",
      " 'delicious' 'sauce' 'great']\n",
      "28 ['mexican' 'burrito' 'chip' 'authentic' 'margarita' 'bean' 'good' 'wa'\n",
      " 'restaurant' 'fresh']\n",
      "29 ['order' 'delivery' 'ordered' 'wa' 'called' 'time' 'phone' 'hour' 'food'\n",
      " 'place']\n",
      "30 ['place' 'good' 'like' 'great' 'time' 'really' 'best' 'owner' 'ive' 'love']\n",
      "31 ['wa' 'food' 'waitress' 'u' 'table' 'server' 'service' 'good' 'time'\n",
      " 'came']\n",
      "32 ['cafe' 'delicious' 'great' 'brunch' 'owner' 'cute' 'place' 'wa' 'outdoor'\n",
      " 'seating']\n",
      "33 ['store' 'selection' 'item' 'parking' 'shop' 'meat' 'price' 'fresh' 'like'\n",
      " 'lot']\n",
      "34 ['ice' 'cream' 'chocolate' 'flavor' 'water' 'kid' 'soft' 'dessert' 'serve'\n",
      " 'topping']\n",
      "35 ['vegan' 'vegetarian' 'delicious' 'option' 'wa' 'try' 'menu' 'amazing'\n",
      " 'im' 'food']\n",
      "36 ['noodle' 'dumpling' 'soup' 'pork' 'dish' 'rice' 'beef' 'spicy'\n",
      " 'restaurant' 'authentic']\n",
      "37 ['italian' 'pasta' 'bread' 'sauce' 'fresh' 'best' 'tomato' 'good' 'great'\n",
      " 'meat']\n",
      "38 ['fry' 'wa' 'sauce' 'hot' 'food' 'good' 'french' 'got' 'order' 'cheese']\n",
      "39 ['tea' 'drink' 'sweet' 'green' 'bun' 'flavor' 'good' 'really' 'dessert'\n",
      " 'like']\n"
     ]
    }
   ],
   "source": [
    "# Examining the top words for each latent feature\n",
    "top_words_index = np.argsort(-H)[:,0:10]\n",
    "most_common_words_per_topic = np.array(words)[top_words_index]\n",
    "for i, items in enumerate(most_common_words_per_topic):\n",
    "    print(i, items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ca8969-d34d-4287-b637-a00ba629e4b1",
   "metadata": {},
   "source": [
    "In looking through the above features, it appears that most of the features have clear categories (e.g. #1 is Pizza, #22 is bar-b-que, #34 is ice cream, etc.). Most of the features have captured cuisine which makes sense as this is the most significant restaurant differentiator as opposed to service quality or location.\n",
    "\n",
    "As there is some overlap here (i.e. #1 is Pizza and #37 is Italian | #31 and #7 are both service-related) it could be that 40 features is too high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7d0f224b-c708-4f14-9b4f-f8135e7c7c0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12641"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f38fa58d-a2c7-4e0d-80cc-9944cb4fbdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [1, 2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f99c077d-e94a-45f3-ab49-babc1cdaa1b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[-3:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "fd8a3666-b744-4c89-986f-ce3e31b6156e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ['without a cue productions', 'wyndham alumnae house', 'zagafen', 'zahav', 'àrdana food  drink']\n",
      "1 ['zesto pizza  grill', 'zio pizza palace  grill', 'zios brick oven pizzeria', 'zoe', 'zuzus kitchen']\n",
      "2 ['odyssey coffee shop', 'reanimator coffee', 'richboro coffee', 'vagrant coffee', 'valerio coffee roasters']\n",
      "3 ['panda pavilion', 'tea garden chinese restaurant', 'temple garden chinese restaurant', 'wing wah kitchen', 'yummi yummi']\n",
      "4 ['zama', 'zento contemporary japanese cuisine', 'zhi izakaya', 'zushi', 'zw sushi land']\n",
      "5 ['writers block rehab', 'ye olde meetinghouse tavern', 'yeats pub', 'yellobar', 'zincbar']\n",
      "6 ['union taco', 'union taco  flourtown', 'unity taqueria', 'vida byob', 'wahoos fish taco']\n",
      "7 ['wawa', 'wawa', 'wawa', 'wawa', 'wendys']\n",
      "8 ['wawa', 'wawa', 'wolfs superior sandwiches', 'wursthaus schmitz', 'yumtown']\n",
      "9 ['wrightstown country store', 'yannis family restaurant', 'yummy 2', 'yummy diner', 'zakes cafe']\n",
      "10 ['wahlburgers', 'wawa', 'wayback burgers', 'wayback burgers', 'zacs hamburgers']\n",
      "11 ['utsav indian cuisine', 'veda  modern indian bistro', 'virundhu', 'zaika', 'zaika kabab  curry']\n",
      "12 ['rubys roof jamaican restaurant', 'soul fed cafe', 'swallow', 'the stand', 'uptown flavors']\n",
      "13 ['wawa', 'wawa', 'wawa', 'wawa', 'yardley bagel cafe']\n",
      "14 ['vietnam express', 'vietnam house', 'vietnam palace', 'yummy pho', 'yunique noodle house']\n",
      "15 ['the night kitchen', 'the ultimate bake shoppe of ardmore', 'the ultimate bake shoppe of wayne', 'tiffanys bakery', 'whats for dessert']\n",
      "16 ['spataros cheesesteaks', 'the original tony lukes', 'tony lukes', 'tony lukes', 'westtown meat market']\n",
      "17 ['white jasmin thai cuisine', 'wild ginger ii', 'wildflowers garden restaurant  thai corner', 'xiandu thai', 'yazmin']\n",
      "18 ['cerise craft steakhouse', 'restaurant ambra', 'studiokitchen', 'village vine']\n",
      "19 ['seafood usa', 'shaking seafood', 'strictly seafood', 'the crab shack 2', 'the crazy leprechaun bar  grill']\n",
      "20 ['wrong crowd beer co', 'wycombe publick house', 'yard house', 'yards brewing company', 'zot']\n",
      "21 ['wingstop', 'wingstop', 'wingstop', 'wingstop', 'wow wings']\n",
      "22 ['woodys', 'woogies woodfired bbq', 'zacharys bbq  soul', 'zig zag bbq', 'zooks bbq barn']\n",
      "23 ['william street common', 'yoris church street bakery', 'yum yum bake shop', 'yum yum bake shops', 'yum yum bake shops']\n",
      "24 ['wishbone', 'wishbone', 'wok  roast', 'yeung cajun grill', 'zara halal food cart']\n",
      "25 ['wendys', 'wendys', 'zara halal food', 'zara halal grill', 'zoes kitchen']\n",
      "26 ['zoes kitchen', 'zoes kitchen', 'zoes kitchen', 'zoes kitchen', 'zoes kitchen']\n",
      "27 ['wawa', 'wawa', 'wawa food markets', 'wok street', 'yoku']\n",
      "28 ['xochitl', 'xolo tacos', 'yankee chipper', 'yosemite burrito', 'zocalo']\n",
      "29 ['two brothers pizza', 'verdis pizza  grill', 'victorias kitchen', 'villagio pizza', 'virginios pizza  grill']\n",
      "30 ['zoes kitchen', 'zorbas opa', 'zorbas taverna', 'zoubi', 'zulyka restaurant and bakery']\n",
      "31 ['yardley inn restaurant  bar', 'yolo', 'york diner restaurant', 'zacharias creek side cafe', 'zotos diner']\n",
      "32 ['treetops kitty cafe', 'waterfront gourmet cafe  deli', 'welcome home cafe', 'west points castaway cafe', 'zoes cafe']\n",
      "33 ['weis markets', 'west bradford market', 'westbrook market', 'wonder foods', 'yohns grocery']\n",
      "34 ['west end alley', 'whats in this', 'winterfell dessert', 'woops', 'yofresh yogurt cafe jenkintown']\n",
      "35 ['virago baking company', 'wiz kid', 'wiz kid', 'yellow bicycle canteen', '¡juice']\n",
      "36 ['zoup', 'zoup', 'zoup', 'zoup', 'zoup']\n",
      "37 ['vita bella', 'vituperio artisan breads  studio', 'viviano', 'willow inn', 'zaccones restaurant']\n",
      "38 ['zacs hamburgers', 'zacs hamburgers', 'zacs hamburgers', 'zesty pizza', 'zio gio']\n",
      "39 ['waterlillies', 'whispering leaves', 'wild violets tea garden cafe', 'zen tea house', 'zhong gang bakery']\n"
     ]
    }
   ],
   "source": [
    "# Identifying the top restaurants for each latent feature\n",
    "rest_dict = defaultdict(list)\n",
    "for index, restaurant in enumerate(W):\n",
    "    key = np.argmax(restaurant)\n",
    "    value = restaurant[key]\n",
    "    name = PA_combined_filtered['name'][index]\n",
    "    rest_dict[key].append([value, name])\n",
    "top_restaurants = defaultdict(list)\n",
    "for feature in rest_dict:\n",
    "    top_restaurants[feature] = list(np.sort(np.array(rest_dict[feature]).T)[1,-5:])\n",
    "for key, value in sorted(top_restaurants.items()):\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd33bf9-be64-4f19-9fa5-23d83f52fabf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
